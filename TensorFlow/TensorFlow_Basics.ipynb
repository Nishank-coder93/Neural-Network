{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import nn as tfnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating List of random data \n",
    "rand_data = np.random.randint(1000,size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([122, 162, 772, ..., 256, 966, 222])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Declaring Tensors\n",
    "\n",
    "* They are primary data structures that TensorFlow uses to operate on the computational graphs \n",
    "* Declare them as variables and feed them to placeholders\n",
    "* There are 4 types of Tensors:\n",
    "    * **Fixed Tensor**\n",
    "    * **Tensors of similar shape**\n",
    "    * **Sequence Tensors**\n",
    "    * **Random Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_dim = 4\n",
    "col_dim = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating Zero filled Tensor\n",
    "zero_tsr = tf.zeros([row_dim,col_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a One filled Tensor\n",
    "ones_tsr = tf.ones([row_dim,col_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a constant filled Tensor\n",
    "filled_tsr = tf.fill([row_dim,col_dim],42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating tensor out of existing constant\n",
    "constant_tsr = tf.constant([1,2,3])\n",
    "\n",
    "# tf.constant can also be used to broadcast constant \n",
    "# into an array mimcking the behaviour of tf.fill()\n",
    "# tf.constant(42,[row_dim,col_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor with similar shape\n",
    "\n",
    "We can initialize tensors based on shape of other tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_similar = tf.zeros_like(constant_tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones_similar = tf.ones_like(constant_tsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Tensors\n",
    "\n",
    "* Tensor flow allows us to specify tensor that contain defined intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following function behaves like range() or numpys linspace() \n",
    "# This example gives [10.0, 11.0, 12.0]\n",
    "linear_tsr = tf.linspace(start=10.0,stop=12.0,num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Like range() \n",
    "# result is [6, 9, 12]\n",
    "integer_seq_tsr = tf.range(start=6,limit=15,delta=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Tensor\n",
    "* Following tensors generate form of random numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generated random numbers from uniform distributions\n",
    "# is of the form minval <= x < maxval (exclusive)\n",
    "randunif_tsr = tf.random_uniform([row_dim, col_dim], minval=0, maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate from normal distribution\n",
    "randnorm_tsr = tf.random_normal([row_dim,col_dim], mean=0.0, stddev=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normal random values that are assured within certain bounds\n",
    "# this function always picks normal values within two stddev of specified mean \n",
    "randnorm_tsr = tf.truncated_normal([row_dim, col_dim], mean=0.0, stddev=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If interested in randomizing entries of arrays\n",
    "# these two functions helps us achieve that\n",
    "# shuffled_output = tf.random_shuffle(input_tensor)\n",
    "# cropped_output = tf.random_crop(input_tensor, crop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sometimes interested in randomly cropping the image of size (height,width,3)\n",
    "# 3 color spectrum\n",
    "# cropped_image = tf.random_crop(my_image, [height/2, width/2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders and Variables\n",
    "\n",
    "* Once we have declared tensors, we would like to create corresponding variables by wrapping in Variable()\n",
    "* Eg. my_var = tf.Variable(tf.zeroes(row_dim,col_dim))\n",
    "* we are not limited to built-in functions\n",
    "    * we can use numpy array to python list or constant\n",
    "    * convert it to a tensor using convert_to_tensor()\n",
    "* Most important distinction to make with Data is whether it is a placeholder or a variable\n",
    "* **Variables** are the parameters of algorithms and TensorFlow keeps track on how to change these to optimize the algo\n",
    "* **Palceholders** are objects that allow you to feed in data of specific type and shape and depend on the result of the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Main way of creating a variable is by using function Variable() that takes in tensor as argument and outputs a variable\n",
    "* Once we declare a variable we still need to initialize it to place it with corresponding method on computational graph \n",
    "* TensorFlow must be informed when it can initialize the variables \n",
    "* While Each variable has a initializer method most common way to do it is use *helper* function \n",
    "    * *tf.global_variable_initializer()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_var = tf.Variable(tf.zeros([row_dim,col_dim]))\n",
    "sess = tf.Session()\n",
    "initialize_op = tf.global_variables_initializer()\n",
    "sess.run(initialize_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Placeholder are holding position for data to be fed into the graph \n",
    "* They get their data from feed_dict argument in the session\n",
    "* Atleast one operation must be performed on the placeholder to put it in the graph \n",
    "* Example below :\n",
    "    * Initialize the graph (sess)\n",
    "    * Declare placeholder x \n",
    "    * Define y as an identity operation on x (basically returns x as it is)\n",
    "    * create data x_vals to feed into placeholder\n",
    "    * run the identity operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95166528,  0.70336717],\n",
       "       [ 0.6986872 ,  0.49709058]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=[2,2])\n",
    "y = tf.identity(x)\n",
    "x_vals = np.random.rand(2,2)\n",
    "sess.run(y, feed_dict={x: x_vals})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Matrices\n",
    "\n",
    "### Creating Matrix\n",
    "\n",
    "We can create two dimensional matrices from numpy arrays or nested list. We can also use tensor creation function and specify the dimensional shape as zeros(), ones(), truncated_normal() etc. TensorFlow also allows us to create diagonal matrix from one-dimensional array list with function diag() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "identity_matrix = tf.diag([1.0,1.0,1.0])\n",
    "A = tf.truncated_normal([2,3])\n",
    "B = tf.fill([2,3], 5.0)\n",
    "C = tf.random_uniform([3,2])\n",
    "D = tf.convert_to_tensor(np.array([[1.,2.,3.],[4.,5.,6.],[7.,8.,9.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(identity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.42246732  0.12971412  0.95483977]\n",
      " [-1.23782456  1.10262001 -1.60466278]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  5.  5.]\n",
      " [ 5.  5.  5.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.56389999  0.79629529]\n",
      " [ 0.65979457  0.99815464]\n",
      " [ 0.36476254  0.07206917]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition, Subtraction, Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.28768134  4.84680271  4.50768471]\n",
      " [ 6.45017672  6.00239658  3.14912653]]\n"
     ]
    }
   ],
   "source": [
    "# Adding two matrices \n",
    "print(sess.run(A+B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Subtracting two matrices\n",
    "print(sess.run(B-B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  5.  5.]\n",
      " [ 5.  5.  5.]]\n"
     ]
    }
   ],
   "source": [
    "# Multiplying two matrices \n",
    "# Matmul() also has an argument to specify whether or not to transpose the argument before multi\n",
    "print(sess.run(tf.matmul(B,identity_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  4.  7.]\n",
      " [ 2.  5.  8.]\n",
      " [ 3.  6.  9.]]\n"
     ]
    }
   ],
   "source": [
    "# To transpose the matrix \n",
    "print(sess.run(tf.transpose(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.66133814775e-16\n"
     ]
    }
   ],
   "source": [
    "# To calculate the determinant \n",
    "print(sess.run(tf.matrix_determinant(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.50359963e+15   9.00719925e+15  -4.50359963e+15]\n",
      " [  9.00719925e+15  -1.80143985e+16   9.00719925e+15]\n",
      " [ -4.50359963e+15   9.00719925e+15  -4.50359963e+15]]\n"
     ]
    }
   ],
   "source": [
    "# To calculate the inverse \n",
    "# The matrix inverse is based on Cholesky decomposition\n",
    "print(sess.run(tf.matrix_inverse(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ -3.15746784,  -0.67276795,  18.8302358 ]), array([[-0.80238891, -0.43402538,  0.40962667],\n",
      "       [-0.16812656,  0.82296167,  0.54264865],\n",
      "       [ 0.57263033, -0.36654613,  0.73330651]]))\n"
     ]
    }
   ],
   "source": [
    "# For Eigenvalues and Eigenvectors\n",
    "# Ouputs the eigenvalues in the first row and the subsequent vectors in the remaining \n",
    "print(sess.run(tf.self_adjoint_eig(D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some TensorFlow operations\n",
    "\n",
    "TensorFlow has standard operations on tensors - **add(), sub(), mul(), div()**.\n",
    "Note that *div()* here returns floor integer value of the result as in Python2 to get the Python3 like result use *truediv()*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(tf.div(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(tf.truediv(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Using floats to get integer division to the nearest rounded down integer\n",
    "print(sess.run(tf.floordiv(3.0,4.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# Get the remainder after the division \n",
    "# mod()\n",
    "print(sess.run(tf.mod(22.0,5.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Cross product between two tensors is achieved by the cross()\n",
    "# It only accepts two three-dimensional vectors\n",
    "print(sess.run(tf.cross([1.,0.,0.],[0.,1.,0.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are some of the other math function that tensor flow provides\n",
    "\n",
    "<img align='center' src='pynb_image/tf_math_func.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are some special math functions that are used in Machine Learning \n",
    "\n",
    "<img align='center' src='pynb_image/tf_special_func.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# It is important to know what functions are available to us so we can generate different custom functions based on this \n",
    "# Example tangent(pi/4) = 1\n",
    "\n",
    "print(sess.run(tf.div(tf.sin(3.1416/4),tf.cos(3.1416/4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Activation Function\n",
    "\n",
    "* When you use Neural Networks we will be using activation functions regularly as they are mandatory part of it\n",
    "* The goal is to adjust the weights and bias\n",
    "* Few main concept of activation function is that they introduce a non-linearity into the graph while normalizing the output\n",
    "* The activation function lives in **Neural Netwrok (nn)** library in TensorFlow. \n",
    "* we can also design our own beside using TensorFlows builtin activation function \n",
    "* import predefined functions using tensorflow.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   3.  10.   0.   5.]\n"
     ]
    }
   ],
   "source": [
    "# Rectified linear unit, known as ReLU is the most basic and common way to introduce a non-linearity into NN \n",
    "# It is continous but not smooth \n",
    "# This function is basically max(0,x)\n",
    "print(sess.run(tfnn.relu([-3.,3.,10.,-5.,5.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  3.  5.  6.  0.  6.]\n"
     ]
    }
   ],
   "source": [
    "# There will be times where we want to cap the linearly increasing part of the preceding ReLU activation function\n",
    "# We can do this my nesting the max(0,x) function in min() function\n",
    "# The implementation that TensorFlow has is called ReLU6 defined by min(max(0,x),6)\n",
    "# This is a version of hard sigmoid function and is computationally faster \n",
    "# This will be handy in CNN and RNN\n",
    "\n",
    "print(sess.run(tfnn.relu6([-3.,3.,5.,10.,-5.,7.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26894143  0.5         0.7310586 ]\n"
     ]
    }
   ],
   "source": [
    "# The sigmoid is the most common continuous function and smooth activation function\n",
    "# also called Logistic function and is of form 1 / (1+exp(-x))\n",
    "# It is not often used because it has the tendency to zero out the backpropogation terms during training \n",
    "\n",
    "print(sess.run(tfnn.sigmoid([-1.,0.,1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.76159418  0.          0.76159418]\n"
     ]
    }
   ],
   "source": [
    "# Another smooth function is the hypertangent this is pretty similar to the sigmoid function\n",
    "# Except instead of having between range 0 and 1 its -1 and 1 \n",
    "# A form of writing that is (exp(x) - exp(-x))/(exp(x)+exp(-x))\n",
    "\n",
    "print(sess.run(tfnn.tanh([-1.,0.,1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5  0.   0.5]\n"
     ]
    }
   ],
   "source": [
    "# The softsign function also gets used as an activation function \n",
    "# The form of this function is x/(abs(x) + 1)\n",
    "# This is supposed to be the continous approximation of sign function \n",
    "\n",
    "print(sess.run(tfnn.softsign([-1., 0., 1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.31326166  0.69314718  0.31326166]\n"
     ]
    }
   ],
   "source": [
    "# Another function is softplus function is a smooth version of ReLU function \n",
    "# This is of the form log(exp(x)+1)\n",
    "\n",
    "print(sess.run(tfnn.softplus([-1.,0.,-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # The softplus goes to infinity as the input increases whereas the softsign goes to 1 \n",
    "# As the input gets smaller the softplus approaches 0 and softsign goes to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.63212055  0.         -0.63212055]\n"
     ]
    }
   ],
   "source": [
    "# The Exponential Linear Unit (ELU) is vvery similar to the softplus function\n",
    "# except the bottom asymptote is -1 instead of 0 \n",
    "# Form is (exp(x) + 1) if x < 0 else x \n",
    "\n",
    "print(sess.run(tfnn.elu([-1., 0., -1.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Activation function introduces non linearity in Neural Network \n",
    "* It is important to note where in the our network we are using activation function \n",
    "* If the activation function has range between 0 and 1(Sigmoid) then comp graph can only output values between 0 and 1\n",
    "* If the activation function is inside and hidden between nodes, then we want to be aware of the effect its range can have on our tensors as we pass them through \n",
    "* If for example the tensors are scaled to be +ve then ideally we would choose activation function that preserves variance in the +ve domain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
